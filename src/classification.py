import os
import pandas as pd

def load_processed_data(export_filename="processed_go.csv"):
    """
    Loads processed gene ontology data from the CSV file in the output folder.
    
    This processed file is expected to be generated by the ingestion pipeline after
    running process_ontology.py. It must contain at least the columns 'gene',
    'pathways', and 'interactions'.
    
    Parameters:
        export_filename (str): Name of the processed CSV file.
        
    Returns:
        pandas.DataFrame: DataFrame containing the processed data.
        
    Raises:
        FileNotFoundError: If the processed CSV file is not found.
    """
    base_dir = os.path.dirname(os.path.abspath(__file__))
    output_path = os.path.join(base_dir, "..", "output", export_filename)
    
    if not os.path.exists(output_path):
        raise FileNotFoundError(f"Processed file not found at: {output_path}")
    
    print(f"Loading processed data from: {output_path}")
    data = pd.read_csv(output_path)
    return data

def compute_impact_index(data):
    """
    Calculates the 'functional impact index' for each gene in the dataset.

    The index is defined as the sum of 'pathways' and 'interactions'.
    This function expects the DataFrame to be fully processed (i.e., by process_ontology.py)
    and to include the required columns.

    Parameters:
        data (pandas.DataFrame): Processed DataFrame containing at least 'pathways'
                                 and 'interactions' columns.

    Returns:
        pandas.DataFrame: DataFrame with an added column 'impact_index'.

    Raises:
        TypeError: If data is not a DataFrame
        KeyError: If the required columns are not present
        ValueError: If data contains invalid values
    """
    if not isinstance(data, pd.DataFrame):
        raise TypeError(f"Expected pandas DataFrame, got {type(data).__name__}")

    if data.empty:
        raise ValueError("Cannot compute impact index on empty DataFrame")

    required_columns = ['pathways', 'interactions']
    for col in required_columns:
        if col not in data.columns:
            raise KeyError(f"Missing required column: {col}")

    # Validate data types and values
    if not pd.api.types.is_numeric_dtype(data['pathways']):
        raise ValueError("Column 'pathways' must be numeric")
    if not pd.api.types.is_numeric_dtype(data['interactions']):
        raise ValueError("Column 'interactions' must be numeric")

    # Check for NaN values
    if data['pathways'].isna().any():
        raise ValueError("Column 'pathways' contains NaN values")
    if data['interactions'].isna().any():
        raise ValueError("Column 'interactions' contains NaN values")

    # Check for negative values
    if (data['pathways'] < 0).any():
        raise ValueError("Column 'pathways' contains negative values")
    if (data['interactions'] < 0).any():
        raise ValueError("Column 'interactions' contains negative values")

    # Create a copy to avoid modifying input
    result = data.copy()
    result['impact_index'] = result['pathways'] + result['interactions']

    return result

def run_classification_pipeline():
    """
    Runs the classification pipeline:
      1. Loads the fully processed data from the output folder (processed_go.csv).
      2. Computes the functional impact index.
      
    Returns:
        pandas.DataFrame: DataFrame with the computed 'impact_index'.
    """
    # Load the processed CSV data (output from process_ontology.py via ingestion.py)
    data = load_processed_data()
    # Compute the impact index
    classified_data = compute_impact_index(data)
    return classified_data

# Example usage:
if __name__ == "__main__":
    try:
        result_df = run_classification_pipeline()
        print("Classification complete. Data with impact index:")
        print(result_df.head())
    except Exception as e:
        print(f"Error during classification: {e}")
